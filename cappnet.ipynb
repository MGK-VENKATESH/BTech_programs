{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9367268,"sourceType":"datasetVersion","datasetId":5680573}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nimport os\nfrom PIL import Image\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n\n## CapsNet layers\n\ndef squash(vectors, axis=-1):\n    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis, keepdims=True)\n    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + 1e-7)\n    return scale * vectors\n\nclass Length(layers.Layer):\n    def call(self, inputs, **kwargs):\n        return tf.sqrt(tf.reduce_sum(tf.square(inputs), -1) + 1e-7)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[:-1]\n\nclass CapsuleLayer(layers.Layer):\n    def __init__(self, num_capsule, dim_capsule, routings=3, **kwargs):\n        super(CapsuleLayer, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n\n    def build(self, input_shape):\n        self.input_num_capsule = input_shape[1]\n        self.input_dim_capsule = input_shape[2]\n        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n                                        self.dim_capsule, self.input_dim_capsule],\n                                 initializer='glorot_uniform',\n                                 name='W')\n\n    def call(self, inputs, training=None):\n        u_hat = tf.einsum('...ji,jik->...jk', inputs, self.W)\n        \n        b = tf.zeros(shape=[tf.shape(inputs)[0], self.input_num_capsule, self.num_capsule])\n        \n        for i in range(self.routings):\n            c = tf.nn.softmax(b, axis=1)\n            outputs = squash(tf.einsum('...ij,...jk->...ik', c, u_hat))\n            if i < self.routings - 1:\n                b += tf.einsum('...ik,...jk->...ij', outputs, u_hat)\n        \n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)\n\n## CapsNet model\n\ndef CapsNet(input_shape, n_class, routings):\n    x = layers.Input(shape=input_shape)\n    \n    # Layer 1: Conv2D\n    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu')(x)\n    \n    # Layer 2: Conv2D\n    conv2 = layers.Conv2D(filters=256, kernel_size=9, strides=2, padding='valid', activation='relu')(conv1)\n    \n    # Layer 3: Primary Caps\n    primarycaps = layers.Conv2D(filters=32, kernel_size=9, strides=2, padding='valid', activation='relu')(conv2)\n    primarycaps_reshaped = layers.Reshape((-1, 8))(primarycaps)\n    primarycaps_squashed = layers.Lambda(squash)(primarycaps_reshaped)\n    \n    # Layer 4: Digit Caps\n    digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings)(primarycaps_squashed)\n    \n    # Layer 5: Output\n    out_caps = Length()(digitcaps)\n    \n    # Models for training and evaluation (prediction)\n    model = models.Model(x, out_caps)\n    \n    return model\n\n## Data Preprocessing\n\ndef load_and_preprocess_data(data_dir, img_size=(64, 64)):\n    classes = ['cloudy', 'desert', 'green_area', 'water']\n    X = []\n    y = []\n    \n    for class_idx, class_name in enumerate(classes):\n        class_dir = os.path.join(data_dir, class_name)\n        for img_name in os.listdir(class_dir):\n            img_path = os.path.join(class_dir, img_name)\n            img = Image.open(img_path).convert('RGB')\n            img = img.resize(img_size)\n            img_array = np.array(img) / 255.0  # Normalize to [0, 1]\n            X.append(img_array)\n            y.append(class_idx)\n    \n    return np.array(X), np.array(y)\n\n# Load and preprocess the data\ndata_dir = '/kaggle/input/remotesensing/Remote sensing satellite images dataset for objects detection/rssid'  # Update this to your Kaggle dataset path\nX, y = load_and_preprocess_data(data_dir)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Convert labels to one-hot encoded format\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\n## Model Configuration\ninput_shape = (64, 64, 3)\nn_class = 4  # cloudy, desert, green_area, water\nroutings = 3\n\n# Create and compile the model\nmodel = CapsNet(input_shape, n_class, routings)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Model summary\nmodel.summary()\n\n## Training\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n\n## Evaluation\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint(f\"Test accuracy: {test_acc:.4f}\")\n\n## Visualization\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n## Predictions\npredictions = model.predict(X_test)\npredicted_classes = np.argmax(predictions, axis=1)\ntrue_classes = np.argmax(y_test, axis=1)\n\n# Confusion Matrix\ncm = confusion_matrix(true_classes, predicted_classes)\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\n# Classification Report\nclass_names = ['cloudy', 'desert', 'green_area', 'water']\nprint(classification_report(true_classes, predicted_classes, target_names=class_names))\n\n# Sample predictions\nn_samples = 5\nsample_indices = np.random.choice(len(X_test), n_samples, replace=False)\n\nplt.figure(figsize=(15, 3))\nfor i, idx in enumerate(sample_indices):\n    plt.subplot(1, n_samples, i+1)\n    plt.imshow(X_test[idx])\n    true_label = class_names[true_classes[idx]]\n    pred_label = class_names[predicted_classes[idx]]\n    plt.title(f\"True: {true_label}\\nPred: {pred_label}\")\n    plt.axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-12T07:47:15.107598Z","iopub.execute_input":"2024-09-12T07:47:15.108006Z","iopub.status.idle":"2024-09-12T07:47:26.793713Z","shell.execute_reply.started":"2024-09-12T07:47:15.107965Z","shell.execute_reply":"2024-09-12T07:47:26.791649Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_39\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_39\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_19 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_58 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m62,464\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_59 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │     \u001b[38;5;34m5,308,672\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_60 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │       \u001b[38;5;34m663,584\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ reshape_19 (\u001b[38;5;33mReshape\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lambda_19 (\u001b[38;5;33mLambda\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ capsule_layer_19 (\u001b[38;5;33mCapsuleLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m16\u001b[0m)          │       \u001b[38;5;34m131,072\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ length_19 (\u001b[38;5;33mLength\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">62,464</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,308,672</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">663,584</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ reshape_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lambda_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ capsule_layer_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CapsuleLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,072</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ length_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Length</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,165,792\u001b[0m (23.52 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,165,792</span> (23.52 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,165,792\u001b[0m (23.52 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,165,792</span> (23.52 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 127\u001b[0m\n\u001b[1;32m    124\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m## Training\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m## Evaluation\u001b[39;00m\n\u001b[1;32m    130\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","Cell \u001b[0;32mIn[25], line 42\u001b[0m, in \u001b[0;36mCapsuleLayer.call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 42\u001b[0m     u_hat \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m...ji,jik->...jk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     b \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mshape(inputs)[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_num_capsule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_capsule])\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroutings):\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling CapsuleLayer.call().\n\n\u001b[1mShape must be rank 3 but is rank 4\n\t for 1th input and equation: ...ji,jik->...jk for '{{node functional_39_1/capsule_layer_19_1/einsum/Einsum}} = Einsum[N=2, T=DT_FLOAT, equation=\"...ji,jik->...jk\"](functional_39_1/lambda_19_1/mul, functional_39_1/capsule_layer_19_1/einsum/Einsum/ReadVariableOp)' with input shapes: [?,256,8], [4,256,16,8].\u001b[0m\n\nArguments received by CapsuleLayer.call():\n  • inputs=tf.Tensor(shape=(None, 256, 8), dtype=float32)\n  • training=True"],"ename":"ValueError","evalue":"Exception encountered when calling CapsuleLayer.call().\n\n\u001b[1mShape must be rank 3 but is rank 4\n\t for 1th input and equation: ...ji,jik->...jk for '{{node functional_39_1/capsule_layer_19_1/einsum/Einsum}} = Einsum[N=2, T=DT_FLOAT, equation=\"...ji,jik->...jk\"](functional_39_1/lambda_19_1/mul, functional_39_1/capsule_layer_19_1/einsum/Einsum/ReadVariableOp)' with input shapes: [?,256,8], [4,256,16,8].\u001b[0m\n\nArguments received by CapsuleLayer.call():\n  • inputs=tf.Tensor(shape=(None, 256, 8), dtype=float32)\n  • training=True","output_type":"error"}]}]}